<!DOCTYPE html>
<html lang="{{ site.lang | default: "en-US" }}">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    <link rel="stylesheet" href ="/assets/css/landing.css">
  </head>
  <body>
    <div class="col-md-5">
      <header>
        <h2><a id = "imp" href="../index.html">Home page</a></h2>
       
        <p>Deconstructing Deep Learning +  δeviations</p>
        
        <p>
          Drop me an <a href = "mailto: msubhaditya@gmail.com">email</a>
          | RSS feed link : <a href ="https://subhadityamukherjee.github.io/feed.xml">Click</a><br>
          Format : 
          Date | Title<br>
          &emsp;&emsp;TL; DR<br>
          <h4>Total posts : 89</h4>
          <div id="search-container">
            Search for something in the blog <input type="text" id="search-input" placeholder="search...">
          </div><br>
          <ul id="results-container"></ul>
        </p>
        
        
        <p class="view">
        <a href="https://www.github.com/SubhadityaMukherjee">View My GitHub Profile </a>
        </p>
      </header>
      
      <hr>
      <script src="/assets/js/search-script.js" type="text/javascript"></script>
      
      <script>
        SimpleJekyllSearch({
          searchInput: document.getElementById('search-input'),
          resultsContainer: document.getElementById('results-container'),
          json: '/assets/search.json'
        })
        </script>
    </div>
<section>
        <div class=col-md-5>
  <p>Notes from 100 Page ML Book</p>
<p>I decided to add notes to this blog too. All such notes will be tagged with "book" for easier search.
This one is my notes while reading "Andriy Burkov : The Hundred-Page Machine Learning Book". <a href="http://themlbook.com/">Amazon</a>. Do support the author if you can.</p>
<p>A quick note on how I make notes. I first annotate the pdf of the book. And then type down the text to make it searchable. Yes I probably could use OCR but this helps me remember more. Also, this is not meant to be comprehensive reviews but only what I find interesting from the book. I read a lot about Deep Learning so these will keep popping up.</p>
<p>Okay now let us get to it :)</p>
<h2 id="initial-thoughts-from-the-content">Initial thoughts from the content</h2>
<ul>
<li>Seems like a book which summarizes ML and tiny bit of DL</li>
<li>Not in depth but more of an executive summary of sorts</li>
<li>Most of the major algorithms explained in brief</li>
<li>Bits of extra information scattered here and there</li>
</ul>
<h2 id="notes">Notes</h2>
<ul>
<li>I skipped making notes of anything I knew prior. So these points are things that I wanted to read again or just found interesting while I was reading the book.</li>
<li>I skipped things like linear regression while making notes so if you dont know what those are better read the book :)</li>
<li>Why ML -&gt; Solve practical problems</li>
</ul>
<h3 id="svm">SVM</h3>
<ul>
<li>SVM sees feature vectors as high dimensional spaces and puts them on a n dimensional plot with an n dimensional hyperplace</li>
<li>minimize euclidean norm</li>
<li>kernels that make boundaries non linear</li>
<li>look for largest margin</li>
<li>Hinge loss -&gt; if data is not linearly separable. penalizes the side of the decision boundary</li>
<li>SVMs with hinge -&gt; soft margin. normal -&gt; hard margin</li>
<li>largin margin : generalization</li>
<li>kernel trick -&gt; implicitly transform original space into a higher dimensional space</li>
<li>lagrange multipliers -&gt; optimization problem by finding equivalent representation -&gt; can be solved by quadratic algos</li>
<li>RBF most widely used</li>
</ul>
<h3 id="random-variable">Random variable</h3>
<ul>
<li>Prob distribution -&gt; list of prov associated with each possible value -&gt; prob mass function</li>
<li>continuous random variable -&gt; inf possible values in interval -&gt; prob density function</li>
<li>expectation -&gt; mean of random variable</li>
</ul>
<h3 id="unbiased-estimator">Unbiased estimator</h3>
<ul>
<li>Unlimited no of unbiased estimators -&gt; mean will give actual value.</li>
</ul>
<h3 id="shallow-learning">Shallow learning</h3>
<ul>
<li>Learns parameters directly from features.</li>
<li>Vs DL -&gt; learnt from outputs of previous layers</li>
</ul>
<h3 id="cost-func">Cost func</h3>
<ul>
<li>avg loss -&gt; empirical risk</li>
</ul>
<h3 id="decision-tree">Decision tree</h3>
<ul>
<li>acyclic graph</li>
<li>in each branch, specific feature is examined</li>
<li>choose next leaf based on threshold</li>
<li>ID3 is approximated by constructing a non parametric model</li>
<li>recursively continue</li>
<li>Entropy is an uncertainty measure -&gt; max when all random values have equal probability</li>
</ul>
<h3 id="gd">GD</h3>
<ul>
<li>SGD -&gt; uses batches to compute gradient </li>
<li>adagrad -&gt; scales ¦Á for each parameter wrt history</li>
<li>momentum -&gt; accelerate SGD</li>
</ul>
<h3 id="techniques">Techniques</h3>
<ul>
<li>Binning -&gt; convert continous feature into multiple binary ones</li>
<li>Normalize -&gt; Increase speed</li>
<li>Standardization -&gt; scale between ¦Ì and ¦Ò</li>
</ul>
<h3 id="data-imputation">Data imputation</h3>
<ul>
<li>same value outside normal range</li>
<li>avg value</li>
<li>use regression to fix</li>
</ul>
<h3 id="regularization">Regularization</h3>
<ul>
<li>L1 -&gt; sparse model,lasso reg</li>
<li>L2 -&gt; feature selection, ridge reg</li>
</ul>
<h3 id="hyper-param">Hyper param</h3>
<ul>
<li>Grid search</li>
<li>Bayesian optimization</li>
<li>Evolutionary optimization</li>
</ul>
<h3 id="rnn">RNN</h3>
<ul>
<li>Sequence</li>
<li>not feed forward -&gt; loops</li>
<li>each unit gets 2 inps -&gt; vector of outputs from prev layer, vector of states from prev time step</li>
<li>backprop through time</li>
<li>gated RNN -&gt; forget gate</li>
<li>store info for future use</li>
<li>read write and erase info stored in units</li>
</ul>
<h3 id="seq2seq">Seq2seq</h3>
<ul>
<li>Encoder -&gt; generate state with meaning representation -&gt; embedding</li>
<li>decoder -&gt; take embedding and give output</li>
<li>best results with attention</li>
</ul>
<h3 id="ensemble">Ensemble</h3>
<ul>
<li>Train many low accuracy models and combine</li>
</ul>
<h3 id="other-learnings">Other learnings</h3>
<ul>
<li>Active learning -&gt; label add to those which contribute most to model. Either density (how many examples around x) or uncertainty (how uncertain prediction of model)</li>
<li>SVM -&gt; Use svm to predict differences and get them annotated</li>
</ul>
<h3 id="semi-supervised">Semi supervised</h3>
<ul>
<li>self learning</li>
<li>autoencoder </li>
<li>bottleneck layer -&gt; embedding</li>
<li>denoising -&gt; corrupts left hand side with random peturbation/ normal gaussian noise</li>
</ul>
<h3 id="zero-shot">Zero shot</h3>
<ul>
<li>use embeddings to represent input x and also output y</li>
</ul>
<h3 id="combine-models">Combine models</h3>
<ul>
<li>Average</li>
<li>majority vote</li>
<li>Stack -&gt; Use stacked model to tune hyper params</li>
</ul>
<h3 id="other-stuff">Other stuff</h3>
<ul>
<li>regularization -&gt; dropout, batch norm, early stop</li>
<li>avoid loops</li>
<li>density estimation -&gt; model probablity density fn -&gt; novelty</li>
<li>DBSCAN -&gt; build clusters with arbitrary shape</li>
<li>Gaussian mixture model -&gt; member of several clusters with diff membership score</li>
<li>UMAP seems to be better then tsne :o</li>
<li>Ranking -&gt; LambdaMart -&gt; optimize lists on metric. eg Mean average precision (MAP)</li>
</ul>
        </div>
  </body>
</html>


<!-- --- -->
<!-- layout: default -->
<!-- --- -->
<!-- <a href = "/deeplearning.html">Go to index</a><br><br> -->
<!--  -->
<!--  -->
<!-- <h1>{{ page.title }}</h1> -->
<!--  -->
<!-- <span class="reading-time" title="Estimated read time"> -->
<!--   {% assign words = content | number_of_words %} -->
<!--   {% if words < 360 %} -->
<!--     <h3>Reading time : ~1 min</h3> -->
<!--   {% else %} -->
<!--     <h3>Reading time : ~{{ words | divided_by:100 }} mins</h3> -->
<!--   {% endif %} -->
<!-- </span> -->
<!--  -->
<!--  -->
<!-- <p class="view">by {{ page.author | default: site.author }}</p> -->
<!-- {% include toc.html html=content %} -->
<!-- {{ content }} -->
<!--  -->
<!-- <section> -->
<!-- Related posts:&emsp; -->
<!-- {% for p in site.related_posts %} -->
<!--   <a href={{ p.url }}> {{ p.title }}&emsp; </a> -->
<!-- {% endfor %} -->
<!--  -->
<!-- </section> -->
<!--  -->
