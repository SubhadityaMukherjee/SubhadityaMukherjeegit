<!DOCTYPE html>
<html lang="{{ site.lang | default: "en-US" }}">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
    <link rel="stylesheet" href ="../assets/css/landing.css">
  </head>
  <body>
    <div class="col-md-5">
      <header>
        <h2><a id = "imp" href="../index.html">Home page</a></h2>
       
        <p>Deconstructing Deep Learning +  δeviations</p>
        
        <p>
          Drop me an <a href = "mailto: msubhaditya@gmail.com">email</a>
          | RSS feed link : <a href ="https://subhadityamukherjee.github.io/feed.xml">Click</a><br>
          Format : 
          Date | Title<br>
          &emsp;&emsp;TL; DR<br>
          <h4>Total posts : 80</h4>
          <!-- <div id="search-container"> -->
          <!--   Search for something in the blog <input type="text" id="search-input" placeholder="search..."> -->
          <!-- </div><br> -->
          <!-- <ul id="results-container"></ul> -->
        </p>
        
        
        <p class="view">
        <a href="https://www.github.com/SubhadityaMukherjee">View My GitHub Profile </a>
        </p>
      </header>
      
      <hr>
      <script src="/assets/js/search-script.js" type="text/javascript"></script>
      
      <script>
        SimpleJekyllSearch({
          searchInput: document.getElementById('search-input'),
          resultsContainer: document.getElementById('results-container'),
          json: '/assets/search.json'
        })
        </script>
    </div>
<section>
        <div class=col-md-5>
                <a href = "../deeplearning.html">Index page</a>
                <br>

  <p><h1>Initialization</h1>

</p>
<p>We explore the different initialization techniques that we have and look at papers to see which does better.</p>
<p>Here goes!</p>
<ul>
<li>Zero Initialization: set all weights to 0
Please dont. I mean its the worst idea. But anyway.</li>
</ul>
<pre><code class="language-julia">W = zeros(2,100)
b = zeros(2)
</code></pre>
<ul>
<li>Normal Initialization: set all weights to random small numbers</li>
</ul>
<p>This is what we did as a test. It does better than init 0 but still. Not a great idea.</p>
<pre><code class="language-julia">W = rand(2,100)
b = rand(2)
</code></pre>
<ul>
<li>Lecun Initialization: normalize variance</li>
</ul>
<p>LeCun, Y. A., Bottou, L., Orr, G. B., &amp; Müller, K. R. (2012). Efficient backprop. In Neural networks: Tricks of the trade (pp. 9-48). Springer, Berlin, Heidelberg.</p>
<p>Since variance grows with number of inputs. This makes it constant xD
It draws samples from a truncated normal distribution centered on 0 with stddev &lt;- sqrt(1 / fan_in) where fan_in is the number of input units in the weight tensor.</p>
<pre><code class="language-julia">using Distributions
lecun_normal(fan_in) = return Distributions.Normal(0, sqrt(1/fan_in))
W = rand(lecun_normal(2), 2, 100)
b = rand(lecun_normal(2), 2)
</code></pre>
<ul>
<li>Xavier Intialization (glorot init)
X. Glorot and Y. Bengio, “Understanding the difficulty of training deep feedforward neural networks,” in International conference on artificial intelligence and statistics, 2010, pp. 249–256.</li>
</ul>
<p>This works better with Sigmoid activations.</p>
<p>There are two of them. Xavier normal and Xavier uniform.
First Xavier Normal - It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.</p>
<pre><code class="language-julia">xavier_normal(fan_in,fan_out) = return Distributions.Normal(0, sqrt(2/(fan_in+fan_out)))

W = rand(xavier_normal(2,100), 2, 100)
b = rand(xavier_normal(2,2), 2)
</code></pre>
<p>Now Xavier Uniform - It draws samples from a uniform distribution within -limit, limit where limit is sqrt(6 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.</p>
<pre><code class="language-julia">function xavier_uniform(fan_in,fan_out)
    limit = sqrt(6/(fan_in+fan_out))
    return Distributions.Uniform(-limit, limit)
end

W = rand(xavier_uniform(2,100), 2, 100)
b = rand(xavier_uniform(2,2), 2)
</code></pre>
<ul>
<li>Kaiming Initialization (he init)</li>
</ul>
<p>K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” arXiv:1502.01852 [cs], Feb. 2015.</p>
<p>This works better with ReLU/Leaky ReLU activations. This is mostly used everywhere because we use ReLU more than Sigmoid now.
Wow. This is just different from Xavier in the fact that there is no fan out :/ And here I thought it was some complicated thing.</p>
<p>He Normal - It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor.</p>
<pre><code class="language-julia">he_normal(fan_in) = return Distributions.Normal(0, sqrt(2/(fan_in)))

W = rand(he_normal(2), 2, 100)
b = rand(he_normal(2), 2)
</code></pre>
<p>He uniform - It draws samples from a uniform distribution within -limit, limit where limit is sqrt(6 / fan_in) where fan_in is the number of input units in the weight tensor.</p>
<pre><code class="language-julia">#export
function he_uniform(fan_in)
    limit = sqrt(6/(fan_in))
    return Distributions.Uniform(-limit, limit)
end

W = rand(he_uniform(2), 2, 100)
b = rand(he_uniform(2), 2)
</code></pre>
<ul>
<li>LSUV
Mishkin, D., &amp; Matas, J. (2015). All you need is a good init. arXiv preprint arXiv:1511.06422.</li>
</ul>
<p>We cannot implement this yet because it requires us to hook into the model while it is training ):
But all it does is when the mean of the current output is &gt; 1e^-3 then we subtract the mean from the bias.
If the current outputs standard deviation -1 is &gt; 1e^-3 then we divide the weight by the standard deviation.</p>
        </div>
  </body>
</html>


<!-- --- -->
<!-- layout: default -->
<!-- --- -->
<!-- <a href = "/deeplearning.html">Go to index</a><br><br> -->
<!--  -->
<!--  -->
<!-- <h1>{{ page.title }}</h1> -->
<!--  -->
<!-- <span class="reading-time" title="Estimated read time"> -->
<!--   {% assign words = content | number_of_words %} -->
<!--   {% if words < 360 %} -->
<!--     <h3>Reading time : ~1 min</h3> -->
<!--   {% else %} -->
<!--     <h3>Reading time : ~{{ words | divided_by:100 }} mins</h3> -->
<!--   {% endif %} -->
<!-- </span> -->
<!--  -->
<!--  -->
<!-- <p class="view">by {{ page.author | default: site.author }}</p> -->
<!-- {% include toc.html html=content %} -->
<!-- {{ content }} -->
<!--  -->
<!-- <section> -->
<!-- Related posts:&emsp; -->
<!-- {% for p in site.related_posts %} -->
<!--   <a href={{ p.url }}> {{ p.title }}&emsp; </a> -->
<!-- {% endfor %} -->
<!--  -->
<!-- </section> -->
<!--  -->
